import numpy as np

class MLP:
    def __init__(self, x, L, M, N, K, type_of_net):
        self.x = x
        
        self.L = L
        self.M = M
        self.N = N
        self.K = K
        
        self.type_of_net = type_of_net

    def ReLU(self, a):
        return np.maximum(0, self.a)

    def softmax(x):
        e_x = np.exp(x)
        
        return e_x / e_x.sum()
    
    def initialize_weights(self):  
        self.list_of_W = []
        
        for i in range(self.L-1):
            if i == 0:
                W = np.random.randn(self.M, self.N)
            elif i == self.L - 1:
                W = np.random.randn(self.N, self.K)
            else:
                W = np.random.randn(self.N, self.N)
            self.list_of_W.append(W)
            
        self.list_of_bias = []

        for i in range(self.L-1):
            if i == 0:
                W = np.random.randn(self.N - 1)
            elif i == self.L - 1:
                W = np.random.randn(self.K - 1)
            else:
                W = np.random.randn(self.M - 1)
            self.list_of_bias.append(b)

        return self.list_of_W, self.list_of_bias
    
    def forward_propagation(self):
        for i in range(self.L - 1):
            if i == 0:
                z_new = x
                z = z_new
            else:
                z_new = ReLU(np.dot(self.list_of_W[i],z) + self.list_of_bias[i])
                z = z_new
                
        if self.type_of_net == "Regressor":
            return z
        elif self.type_of_net == "Classifier":
            return softmax(z)
        
# Входной вектор    
x = np.random.randn(5)
# Количество слоев
L = 4
# Нейронов во входном слое
M = x.shape[0]
# Нейронов в скрытых слоях
N = 5
# Нейронов в выходном слое
K = 3

myMLP = MLP(x, L, M, N, K, "Classifier")

myMLP.initialize_weights()

result = myMLP.forward_propagation()
print(result)
